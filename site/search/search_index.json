{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FairBench A comprehensive AI fairness exploration framework. Author: Emmanouil (Manios) Krasanakis License: Apache Software License 2 Fairness reports and stamps Multivalue multiattribute Measure building blocks ML integration Recipe notebooks can be found in the project's github page .","title":"Home"},{"location":"#fairbench","text":"A comprehensive AI fairness exploration framework. Author: Emmanouil (Manios) Krasanakis License: Apache Software License 2 Fairness reports and stamps Multivalue multiattribute Measure building blocks ML integration Recipe notebooks can be found in the project's github page .","title":"FairBench"},{"location":"quickstart/","text":"Quickstart Install FairBench with: pip install --upgrade fairbench A typical workflow involves the following steps: Produce test or validation data Declare (multi-attribute multi-value) sensitive attributes as forks Create and explore reports that present many types of biases Extract relevant fairness definitions into model cards We will show how to investigate the fairness of a binary classification algorithm. Other types of systems can be analysed too. Import the library and create some predictions: from sklearn.linear_model import LogisticRegression classifier = LogisticRegression () classifier . fit ( trainx , trainy ) yhat = classifier . predict ( x ) Declare sensitive attributes for test data with a data structure called fork . FairBench supports multi-value and multi-attribute fairness analysis with the same interfaces by just adding more attributes to the same fork. Forks can be constructed with many patterns, depending on available datatypes. Given that a sensitive attribute fork has been created, use it alongside predictions to generate a fairness report . Here, we will generate a multireport, which is the most general -albeit a little complex- deterministic fairness exploration FairBench provides. We set some test and predictive data arguments with keywords pertaining to the classification task at hand; other arguments enable metrics for other tasks. sensitive = fb . Fork ( men = sensitive1 , case2 = sensitive2 ) report = fb . multireport ( predictions = yhat , labels = y , sensitive = sensitive ) fb . describe ( report ) # or print(report) or fb.visualize(report) or fb.interactive(report) Either perform an interactive exploration of the report to get a sense of where unfairness is exhibited, or create some stamps about fairness definitions it adheres to and use these to generate a fairness model card . The snippet below exports a model card to an html file format and opens it in your browser. The card will look like this . You can omit the file export or show arguments, or export cards in markdown or yaml formats. stamps = fb . combine ( fb . stamps . prule ( report ), fb . stamps . accuracy ( report ), fb . stamps . four_fifths_rule ( report ) ) fb . modelcards . tohtml ( stamps , file = \"output.html\" , show = True ) Tip Multireport considers many possible variations of what could constitute bias. For naive analysis, create a binreport instead. Danger Always consult stakeholders to decide which stamps are relevant for your systems.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Install FairBench with: pip install --upgrade fairbench A typical workflow involves the following steps: Produce test or validation data Declare (multi-attribute multi-value) sensitive attributes as forks Create and explore reports that present many types of biases Extract relevant fairness definitions into model cards We will show how to investigate the fairness of a binary classification algorithm. Other types of systems can be analysed too. Import the library and create some predictions: from sklearn.linear_model import LogisticRegression classifier = LogisticRegression () classifier . fit ( trainx , trainy ) yhat = classifier . predict ( x ) Declare sensitive attributes for test data with a data structure called fork . FairBench supports multi-value and multi-attribute fairness analysis with the same interfaces by just adding more attributes to the same fork. Forks can be constructed with many patterns, depending on available datatypes. Given that a sensitive attribute fork has been created, use it alongside predictions to generate a fairness report . Here, we will generate a multireport, which is the most general -albeit a little complex- deterministic fairness exploration FairBench provides. We set some test and predictive data arguments with keywords pertaining to the classification task at hand; other arguments enable metrics for other tasks. sensitive = fb . Fork ( men = sensitive1 , case2 = sensitive2 ) report = fb . multireport ( predictions = yhat , labels = y , sensitive = sensitive ) fb . describe ( report ) # or print(report) or fb.visualize(report) or fb.interactive(report) Either perform an interactive exploration of the report to get a sense of where unfairness is exhibited, or create some stamps about fairness definitions it adheres to and use these to generate a fairness model card . The snippet below exports a model card to an html file format and opens it in your browser. The card will look like this . You can omit the file export or show arguments, or export cards in markdown or yaml formats. stamps = fb . combine ( fb . stamps . prule ( report ), fb . stamps . accuracy ( report ), fb . stamps . four_fifths_rule ( report ) ) fb . modelcards . tohtml ( stamps , file = \"output.html\" , show = True ) Tip Multireport considers many possible variations of what could constitute bias. For naive analysis, create a binreport instead. Danger Always consult stakeholders to decide which stamps are relevant for your systems.","title":"Quickstart"},{"location":"advanced/distributed/","text":"Computational branches Multiple forked variables Parallel & distributed computing Multiple forked variables If you have multiple forks , they should all have the same branches. Each branch will execute independently of the rest (non-fork inputs are shared between them). You can even create machine learning model forks, where a different model is applied on different branches: from sklearn.linear_model import LogisticRegression , MLPClassifier x , y = ... classifier = fb . Fork ( case1 = LogisticRegression (), case2 = MLPClassifier ()) classifier = classifier . fit ( x , y ) yhat = classifier . predict ( x ) Forks automatically try to call wrapped class methods, i.e., classifier.fit is also a fork whose branches hold the outcome of applying fit on each branch's model. The inputs x,y could also have been forks, in which case each branch would have been trained on respective values. Recall that branch values can be accessed via class fields, for example like yhat = (yhat.case1+yhat.case2)/2 . This computation produces a factual value that is not bound to any branch. On the other hand yhat.case1 and yhat.case2 would be used during assessment of case1 sensitive attribute values and case2 sensitive attribute values. print ( classifier ) # case2: MLPClassifier() # case1: LogisticRegression() print ( yhat ) # case2: [0 1 0 1 0 1 1 1] # case1: [0 1 0 1 1 1 1 1] print (( yhat . case1 + yhat . case2 ) / 2 ) # [0. 1. 0. 1. 0.5 1. 1. 1. ] A visual view of how data are organized across branches follows. Some variables are identical but others obtain different values per branch. The same code is run on all branches concurrently and independently. Tip Use branches to run several computation pipelines concurrently. Danger Avoid overlapping names between branches and class fields or methods, as they are both accessed with the same annotation. If there is confusion, branch values will be obtained. Parallel & distributed computing FairBench supports parallel or distributed computing of computationally intensive operations that are separated into different branches via dask . This capability can be enabled per: import fairbench as fb fb . distributed ( * args , ** todict ) where the arguments and keyword arguments are those necessary to instantiate a dask.Client . For example, you can provide no arguments to start simple parallel computing, where workers are created locally in your machine. You can also provide an IP address pointing to the dask server. If the server's workers have been instantiated with the same names as some branches, those branches will be executed there in respective servers. Warning If computations are too simple, parallelization will be slower, due to data transfer overheads. Warning Accessing branch values, for instance in report generation and visualization, under distributed computing awaits for dependent remote computations to conclude.","title":"Computational branches"},{"location":"advanced/distributed/#computational-branches","text":"Multiple forked variables Parallel & distributed computing","title":"Computational branches"},{"location":"advanced/distributed/#multiple-forked-variables","text":"If you have multiple forks , they should all have the same branches. Each branch will execute independently of the rest (non-fork inputs are shared between them). You can even create machine learning model forks, where a different model is applied on different branches: from sklearn.linear_model import LogisticRegression , MLPClassifier x , y = ... classifier = fb . Fork ( case1 = LogisticRegression (), case2 = MLPClassifier ()) classifier = classifier . fit ( x , y ) yhat = classifier . predict ( x ) Forks automatically try to call wrapped class methods, i.e., classifier.fit is also a fork whose branches hold the outcome of applying fit on each branch's model. The inputs x,y could also have been forks, in which case each branch would have been trained on respective values. Recall that branch values can be accessed via class fields, for example like yhat = (yhat.case1+yhat.case2)/2 . This computation produces a factual value that is not bound to any branch. On the other hand yhat.case1 and yhat.case2 would be used during assessment of case1 sensitive attribute values and case2 sensitive attribute values. print ( classifier ) # case2: MLPClassifier() # case1: LogisticRegression() print ( yhat ) # case2: [0 1 0 1 0 1 1 1] # case1: [0 1 0 1 1 1 1 1] print (( yhat . case1 + yhat . case2 ) / 2 ) # [0. 1. 0. 1. 0.5 1. 1. 1. ] A visual view of how data are organized across branches follows. Some variables are identical but others obtain different values per branch. The same code is run on all branches concurrently and independently. Tip Use branches to run several computation pipelines concurrently. Danger Avoid overlapping names between branches and class fields or methods, as they are both accessed with the same annotation. If there is confusion, branch values will be obtained.","title":"Multiple forked variables"},{"location":"advanced/distributed/#parallel-distributed-computing","text":"FairBench supports parallel or distributed computing of computationally intensive operations that are separated into different branches via dask . This capability can be enabled per: import fairbench as fb fb . distributed ( * args , ** todict ) where the arguments and keyword arguments are those necessary to instantiate a dask.Client . For example, you can provide no arguments to start simple parallel computing, where workers are created locally in your machine. You can also provide an IP address pointing to the dask server. If the server's workers have been instantiated with the same names as some branches, those branches will be executed there in respective servers. Warning If computations are too simple, parallelization will be slower, due to data transfer overheads. Warning Accessing branch values, for instance in report generation and visualization, under distributed computing awaits for dependent remote computations to conclude.","title":"Parallel &amp; distributed computing"},{"location":"advanced/manipulation/","text":"Report manipulation Manipulating metrics Reduction Combining and comparing reports Tip To extract popular fairness definitions from reports, process them with stamps . Editing metrics Reports are forks of dictionaries and you can use normal dictionary methods to access and edit their elements (given that forks provide access to any possible methods of internal objects). For instance, you can use the following code to calculate a notion of total disparate mistreatment as the sum of dfnr and dfpr of a binary report in all brnaches and remove these entries from all branch dictionaries using Python's dictionary entry deletion: import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . binreport ( predictions =... , labels =... , sensitive = sensitive ) fb . describe ( report ) report [ \"mistreatment\" ] = abs ( report . dfpr ) + abs ( report . dfnr ) del report [ \"dfpr\" ] del report [ \"dfnr\" ] fb . describe ( report ) This will print the following to the console: Metric case2 case1 accuracy 0.938 0.938 prule 0.667 0.571 dfpr 0.056 0.071 dfnr 0.167 0.500 Metric case2 case1 accuracy 0.938 0.938 prule 0.667 0.571 mistreatment 0.222 0.571 Reduction Reports can be reduced alongside branches. Again, this operation is in general applicable to all variable forks, although this time usage is discouraged outside of report manipulation, as reduction creates new -and potentially unforeseen- data branches, but constitutes the main mechanism for summarizing multi-attribute reports into one measure. Reduction internally runs three types of functions obtained from its arguments: transform values found in the report for each metric, which can be either None or abs . expand the list of branch values for each metric, namely None , a pairwise ratio between values, or absolute diff erences between branch values. reducer method that takes a list of all branch values for each metric and summarizes them into one value. These can be mean,max,min,sum,budget , where the last one is the logarithm of the maximum declared in differential fairness formulations. To demonstrate usage, we compute the mean, and budget of the absolute value ratio via the following code. Reduction creates new reports that comprise only one branch. The branch's name is dynamically derived by parameters (e.g., \"budgetratioabs\" ), but you can also set a specific one with the argument name=\"ReductionName\" . import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . accreport ( predictions =... , labels =... , sensitive = sensitive ) mean_across_branches = fb . reduce ( report , fb . mean , name = \"avg\" ) max_abs_across_branches = fb . reduce ( report , fb . budget , expand = fb . ratio , transform = fb . abs ) Tip You will typically want to perform custom reductions on an accreport or on manually generated reports for some base evaluation measures. Combine the outcome of more than one reduction for better understanding. Info Call areduce with the same arguments to obtain the reduction's numeric output instead of a fork. Combining and comparing reports Reports, including reduced ones, can be combined to create a super-report with all sub-branches. This is demonstrated in the following snippet: new_report = fb . combine ( report , mean_across_branches , max_abs_across_branches ) fb . describe ( new_report ) Metric case2 case1 avg budgetratioabs accuracy 0.938 0.938 0.938 0.000 fpr 0.056 0.071 0.063 0.251 fnr 0.167 0.500 0.333 1.099 Sometimes, you may want to compare the same report generated for multiple algorithms. To do this, you need to generate a fork where each branch holds a respective algorithm's default. Then, you can extract and combine values for each metric as shown in the following snipper: reports = fb . Fork ( ppr = report , lfpro = fair_report ) rep = fb . extract ( acc = reports . mean . accuracy , prule = reports . minratio . pr ) fb . describe ( rep ) Metric ppr lfpro acc 0.819 0.826 prule 0.261 0.957 When extracting values from reports, you can optionally omit the final value getter as long as is the same as the new name. For example, fb.extract(accuracy=reports.mean) is equivalent to fb.extract(accuracy=reports.mean.accuracy) given that reports.mean also returns a fork.","title":"Report manipulation"},{"location":"advanced/manipulation/#report-manipulation","text":"Manipulating metrics Reduction Combining and comparing reports Tip To extract popular fairness definitions from reports, process them with stamps .","title":"Report manipulation"},{"location":"advanced/manipulation/#editing-metrics","text":"Reports are forks of dictionaries and you can use normal dictionary methods to access and edit their elements (given that forks provide access to any possible methods of internal objects). For instance, you can use the following code to calculate a notion of total disparate mistreatment as the sum of dfnr and dfpr of a binary report in all brnaches and remove these entries from all branch dictionaries using Python's dictionary entry deletion: import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . binreport ( predictions =... , labels =... , sensitive = sensitive ) fb . describe ( report ) report [ \"mistreatment\" ] = abs ( report . dfpr ) + abs ( report . dfnr ) del report [ \"dfpr\" ] del report [ \"dfnr\" ] fb . describe ( report ) This will print the following to the console: Metric case2 case1 accuracy 0.938 0.938 prule 0.667 0.571 dfpr 0.056 0.071 dfnr 0.167 0.500 Metric case2 case1 accuracy 0.938 0.938 prule 0.667 0.571 mistreatment 0.222 0.571","title":"Editing metrics"},{"location":"advanced/manipulation/#reduction","text":"Reports can be reduced alongside branches. Again, this operation is in general applicable to all variable forks, although this time usage is discouraged outside of report manipulation, as reduction creates new -and potentially unforeseen- data branches, but constitutes the main mechanism for summarizing multi-attribute reports into one measure. Reduction internally runs three types of functions obtained from its arguments: transform values found in the report for each metric, which can be either None or abs . expand the list of branch values for each metric, namely None , a pairwise ratio between values, or absolute diff erences between branch values. reducer method that takes a list of all branch values for each metric and summarizes them into one value. These can be mean,max,min,sum,budget , where the last one is the logarithm of the maximum declared in differential fairness formulations. To demonstrate usage, we compute the mean, and budget of the absolute value ratio via the following code. Reduction creates new reports that comprise only one branch. The branch's name is dynamically derived by parameters (e.g., \"budgetratioabs\" ), but you can also set a specific one with the argument name=\"ReductionName\" . import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . accreport ( predictions =... , labels =... , sensitive = sensitive ) mean_across_branches = fb . reduce ( report , fb . mean , name = \"avg\" ) max_abs_across_branches = fb . reduce ( report , fb . budget , expand = fb . ratio , transform = fb . abs ) Tip You will typically want to perform custom reductions on an accreport or on manually generated reports for some base evaluation measures. Combine the outcome of more than one reduction for better understanding. Info Call areduce with the same arguments to obtain the reduction's numeric output instead of a fork.","title":"Reduction"},{"location":"advanced/manipulation/#combining-and-comparing-reports","text":"Reports, including reduced ones, can be combined to create a super-report with all sub-branches. This is demonstrated in the following snippet: new_report = fb . combine ( report , mean_across_branches , max_abs_across_branches ) fb . describe ( new_report ) Metric case2 case1 avg budgetratioabs accuracy 0.938 0.938 0.938 0.000 fpr 0.056 0.071 0.063 0.251 fnr 0.167 0.500 0.333 1.099 Sometimes, you may want to compare the same report generated for multiple algorithms. To do this, you need to generate a fork where each branch holds a respective algorithm's default. Then, you can extract and combine values for each metric as shown in the following snipper: reports = fb . Fork ( ppr = report , lfpro = fair_report ) rep = fb . extract ( acc = reports . mean . accuracy , prule = reports . minratio . pr ) fb . describe ( rep ) Metric ppr lfpro acc 0.819 0.826 prule 0.261 0.957 When extracting values from reports, you can optionally omit the final value getter as long as is the same as the new name. For example, fb.extract(accuracy=reports.mean) is equivalent to fb.extract(accuracy=reports.mean.accuracy) given that reports.mean also returns a fork.","title":"Combining and comparing reports"},{"location":"advanced/metrics/","text":"Performance metrics FairBench reports implement several definitions of fairness that quantify imbalances between groups of people (e.g., different genders) in terms of them obtaining different assessments by base performance metrics. These assessments are often further reduced across groups of samples with different sensitive attribute values. Here, we present base metrics used to assess AI that reports use. All metrics computed on a subset of 'sensitive samples', which form the group being examined each time. Outputs are wrapped into explainable objects that keep track of relevant metadata. Classification Ranking Regression Classification Classification metrics assess binary predictions. Unless stated otherwise, the following arguments need to be provided: Argument Role Values predictions system output binary array labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy) accuracy Computes the accuracy for correctly predicting provided binary labels for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, true predictions pr Computes the positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . This metric does not use the labels argument. Explanation: number of samples, positive predictions positives Computes the number of positive predictions for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . This metric does not use the labels argument. Explanation: number of samples tpr Computes the true positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of positives, number of true positives tnr Computes the true negative rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of negatives, number of true negatives fpr Computes the false positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of positives, number of false positives fnr Computes the false negative rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of negatives, number of false negatives Ranking Ranking metrics assess scores that aim to approach provided labels. The following arguments need to be provided: Argument Role Values scores system output array with elements in \\([0,1]\\) labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy) auc Computes the area under curve of the receiver operating characteristics for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, the receiver operating characteristic curve phi Computes the score mass of sensitive data samples compared to the total scores. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, sensitive scores hr Computes the hit rate, i.e., precision, for a set number of top scores for sensitive data samples. This is used to assess recommendation systems. By default, the top-3 hit rate is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\) reck Computes the recall for a set number of top scores for sensitive data samples. This is used to assess recommendation systems. By default, the top-3 recall is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\) f1k Computes the f1-score for a set number of top scores for sensitive data samples. This is the harmonic mean between hr and preck and is used to assess recommendation systems. By default, the top-3 f1 is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\) ap Computes the average hit rate/precession across different numbers of top scores with correct predictions. By default, the top-3 average precision is computed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, hr curve Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\) Regression Regression metrics assess scores that aim to reproduce desired target scores. The following arguments need to be provided: Argument Role Values scores system output any float array targets prediction target any float array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy) max_error Computes the maximum absolute error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: --- mae Computes the mean of the absolute error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of absolute errors mse Computes the mean of the square error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of square errors rmse Computes the root of mse. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of square errors r2 Computes the r2 score between scores and target values, adjusted for the provided degree of freedom (default is zero). Returns a float in the range \\((-\\infty,1]\\) , where larger values correspond to better estimation and models that output the mean are evaluated to zero. Explanation: number of samples, sum of square errors, degrees of freedom Additional argument Role Values deg_freedom parameter integer in the range \\([0,\\infty)\\) pinball Computes the pinball deviation between scores and target values for a balance parameter alpha (default is 0.5). Returns a float in the range \\([0,\\infty)\\) , where smaller values correspond to better estimation. Explanation: number of samples Additional argument Role Values alpha parameter float in the range \\([0,1]\\)","title":"Performance metrics"},{"location":"advanced/metrics/#performance-metrics","text":"FairBench reports implement several definitions of fairness that quantify imbalances between groups of people (e.g., different genders) in terms of them obtaining different assessments by base performance metrics. These assessments are often further reduced across groups of samples with different sensitive attribute values. Here, we present base metrics used to assess AI that reports use. All metrics computed on a subset of 'sensitive samples', which form the group being examined each time. Outputs are wrapped into explainable objects that keep track of relevant metadata. Classification Ranking Regression","title":"Performance metrics"},{"location":"advanced/metrics/#classification","text":"Classification metrics assess binary predictions. Unless stated otherwise, the following arguments need to be provided: Argument Role Values predictions system output binary array labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy)","title":"Classification"},{"location":"advanced/metrics/#accuracy","text":"Computes the accuracy for correctly predicting provided binary labels for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, true predictions","title":"accuracy"},{"location":"advanced/metrics/#pr","text":"Computes the positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . This metric does not use the labels argument. Explanation: number of samples, positive predictions","title":"pr"},{"location":"advanced/metrics/#positives","text":"Computes the number of positive predictions for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . This metric does not use the labels argument. Explanation: number of samples","title":"positives"},{"location":"advanced/metrics/#tpr","text":"Computes the true positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of positives, number of true positives","title":"tpr"},{"location":"advanced/metrics/#tnr","text":"Computes the true negative rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of negatives, number of true negatives","title":"tnr"},{"location":"advanced/metrics/#fpr","text":"Computes the false positive rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of positives, number of false positives","title":"fpr"},{"location":"advanced/metrics/#fnr","text":"Computes the false negative rate of binary predictions for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, number of negatives, number of false negatives","title":"fnr"},{"location":"advanced/metrics/#ranking","text":"Ranking metrics assess scores that aim to approach provided labels. The following arguments need to be provided: Argument Role Values scores system output array with elements in \\([0,1]\\) labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy)","title":"Ranking"},{"location":"advanced/metrics/#auc","text":"Computes the area under curve of the receiver operating characteristics for sensitive data samples. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, the receiver operating characteristic curve","title":"auc"},{"location":"advanced/metrics/#phi","text":"Computes the score mass of sensitive data samples compared to the total scores. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, sensitive scores","title":"phi"},{"location":"advanced/metrics/#hr","text":"Computes the hit rate, i.e., precision, for a set number of top scores for sensitive data samples. This is used to assess recommendation systems. By default, the top-3 hit rate is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\)","title":"hr"},{"location":"advanced/metrics/#reck","text":"Computes the recall for a set number of top scores for sensitive data samples. This is used to assess recommendation systems. By default, the top-3 recall is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\)","title":"reck"},{"location":"advanced/metrics/#f1k","text":"Computes the f1-score for a set number of top scores for sensitive data samples. This is the harmonic mean between hr and preck and is used to assess recommendation systems. By default, the top-3 f1 is analysed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, true top scores Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\)","title":"f1k"},{"location":"advanced/metrics/#ap","text":"Computes the average hit rate/precession across different numbers of top scores with correct predictions. By default, the top-3 average precision is computed. Returns a float in the range \\([0,1]\\) . Explanation: number of samples, top scores, hr curve Additional argument Role Values top parameter integer in the range \\([1,\\infty)\\)","title":"ap"},{"location":"advanced/metrics/#regression","text":"Regression metrics assess scores that aim to reproduce desired target scores. The following arguments need to be provided: Argument Role Values scores system output any float array targets prediction target any float array sensitive sensitive attribute fork of arrays with elements in \\([0,1]\\) (either binary or fuzzy)","title":"Regression"},{"location":"advanced/metrics/#max_error","text":"Computes the maximum absolute error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: ---","title":"max_error"},{"location":"advanced/metrics/#mae","text":"Computes the mean of the absolute error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of absolute errors","title":"mae"},{"location":"advanced/metrics/#mse","text":"Computes the mean of the square error between scores and targets for sensitive data samples. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of square errors","title":"mse"},{"location":"advanced/metrics/#rmse","text":"Computes the root of mse. Returns a float in the range \\([0,\\infty)\\) . Explanation: number of samples, sum of square errors","title":"rmse"},{"location":"advanced/metrics/#r2","text":"Computes the r2 score between scores and target values, adjusted for the provided degree of freedom (default is zero). Returns a float in the range \\((-\\infty,1]\\) , where larger values correspond to better estimation and models that output the mean are evaluated to zero. Explanation: number of samples, sum of square errors, degrees of freedom Additional argument Role Values deg_freedom parameter integer in the range \\([0,\\infty)\\)","title":"r2"},{"location":"advanced/metrics/#pinball","text":"Computes the pinball deviation between scores and target values for a balance parameter alpha (default is 0.5). Returns a float in the range \\([0,\\infty)\\) , where smaller values correspond to better estimation. Explanation: number of samples Additional argument Role Values alpha parameter float in the range \\([0,1]\\)","title":"pinball"},{"location":"advanced/ml_integration/","text":"ML Integration FairBench integrates smoothly into machine learning workflows by being able to process their primitives and providing backpropagate-able computations. This is achieved by running eagerpy under the hood to abstract the operations of different frameworks with the same interfaces. Backend selection Buffering batch predictions Backend selection If you provide primitives of various frameworks (e.g., tensors), these will be internally converted to FairBench's selected backend. Switch to a different backend as shown below. Info The numpy backend is selected by default. import fairbench as fb backend_name = \"numpy\" # or \"torch\", \"tensorflow\", \"jax\" fb . setbackend ( backend_name ) For simple fairness/bias quantification, it does not matter how computations run internally. However, you may want to backpropagate results, for example to add them to the loss. In this case, you need to set internal operations to run on the same backend as the one that runs your AI. Buffering batch predictions When training machine learning algorithms, you may want to concatenate the same report arguments generated across several batches. You can keep arguments by calling fairbench.todict to convert a set of keyword arguments to a fork of dictionaries, which reports automatically unpack internally. Entries of such forks (e.g., predictions in the example below) can be concatenated via a namesake method. iteratively concatenate such dictionaries with previous concatenation outcomes to generate a final dictionary of keyword arguments to pass to reports like so: data = None for batch in range ( batches ): yhat , y , sensitive = ... # compute for the batch data = fb . concatenate ( data , fb . todict ( predictions = yhat , labels = y , sensitive = sensitive )) report = fb . multireport ( data )","title":"ML Integration"},{"location":"advanced/ml_integration/#ml-integration","text":"FairBench integrates smoothly into machine learning workflows by being able to process their primitives and providing backpropagate-able computations. This is achieved by running eagerpy under the hood to abstract the operations of different frameworks with the same interfaces. Backend selection Buffering batch predictions","title":"ML Integration"},{"location":"advanced/ml_integration/#backend-selection","text":"If you provide primitives of various frameworks (e.g., tensors), these will be internally converted to FairBench's selected backend. Switch to a different backend as shown below. Info The numpy backend is selected by default. import fairbench as fb backend_name = \"numpy\" # or \"torch\", \"tensorflow\", \"jax\" fb . setbackend ( backend_name ) For simple fairness/bias quantification, it does not matter how computations run internally. However, you may want to backpropagate results, for example to add them to the loss. In this case, you need to set internal operations to run on the same backend as the one that runs your AI.","title":"Backend selection"},{"location":"advanced/ml_integration/#buffering-batch-predictions","text":"When training machine learning algorithms, you may want to concatenate the same report arguments generated across several batches. You can keep arguments by calling fairbench.todict to convert a set of keyword arguments to a fork of dictionaries, which reports automatically unpack internally. Entries of such forks (e.g., predictions in the example below) can be concatenated via a namesake method. iteratively concatenate such dictionaries with previous concatenation outcomes to generate a final dictionary of keyword arguments to pass to reports like so: data = None for batch in range ( batches ): yhat , y , sensitive = ... # compute for the batch data = fb . concatenate ( data , fb . todict ( predictions = yhat , labels = y , sensitive = sensitive )) report = fb . multireport ( data )","title":"Buffering batch predictions"},{"location":"basics/forks/","text":"Forks Forks declare variables with several named values, called branches. Branches are typically binary arrays of attribute values. For instance, there may a branch for each gender or race to capture which data samples exhibit those protected attribute values. Fork definition Intersectional analysis Fork definition Generate forks by passing keyword arguments to a constructor. In the snippet below men , women , and nonbinary are branch names. Branch values can be anything, though they will usually be arrays. Provide any number of branches with any names and access their values like object members, as shown below. Info When working with specific backends , branch values are internally converted to appropriate data types (e.g., arrays or tensors). import fairbench as fb import numpy as np sensitive = fb . Fork ( men = np . array ([ 1 , 1 , 0 , 0 , 0 ]), women = np . array ([ 0 , 0 , 1 , 1 , 0 ]), nonbinary = np . array ([ 0 , 0 , 0 , 0 , 1 ])) print ( sensitive . nonbinary ) # [0, 0, 0, 0, 1] To set some (or all) branches programmatically pass a dictionary as a positional argument like so: sensitive = fb . Fork ({ \"non-binary\" : np . array ([ 0 , 0 , 0 , 0 , 1 ])}, men = np . array ([ 1 , 1 , 0 , 0 , 0 ]), women = np . array ([ 0 , 0 , 1 , 1 , 0 ])) To create forks by analysing categorical values found in iterables prepend the latter with the categories@ operator. For example, the following code snippet creates two branches genderMan,genderWoman and stores binary membership to each of those. fork = fb . Fork ( gender = fb . categories @ [ \"Man\" , \"Woman\" , \"Man\" , \"Woman\" , \"Nonbin\" ]) print ( fork ) # genderMan: [1, 0, 1, 0, 0] # genderWoman: [0, 1, 0, 1, 0] # genderNonbin: [0, 0, 0, 0, 1] Add the outcomes of any number of category analyses to a fork. Use positional arguments (instead of named keyword arguments, such as gender in the above example) to avoid prepending the keyword argument's name to branch names. Any iterable can be analysed into categories instead of a list, including categorical tensors or arrays. Intersectional analysis For more than one sensitive attribute, add the branches you would declare for every attribute more branches of the same fork. For instance, the following is a valid fork that considers three gender attribute values and one binary sensitive attribute value for young vs old people: import numpy as np sensitive = fb . Fork ( fb . categories @ [ \"Man\" , \"Woman\" , \"Man\" , \"Woman\" , \"Nonbin\" ], IsOld = np . array ([ 0 , 1 , 0 , 1 , 0 ])) # Man: [1, 0, 1, 0, 0] # Woman: [0, 1, 0, 1, 0] # Nonbin: [0, 0, 0, 0, 1] # IsOld0.0 [1, 0, 0, 0, 1] # IsOld1.0 [0, 1, 1, 1, 0] For more than one sensitive attributes, branches capturing the values of different attributes will have overlapping non-zeroes. Thus, you might want to consider intersectional instead of more naive definitions of fairness by creating all branch combinations with at least one data sample per: sensitive = sensitive . intersectional ()","title":"Forks"},{"location":"basics/forks/#forks","text":"Forks declare variables with several named values, called branches. Branches are typically binary arrays of attribute values. For instance, there may a branch for each gender or race to capture which data samples exhibit those protected attribute values. Fork definition Intersectional analysis","title":"Forks"},{"location":"basics/forks/#fork-definition","text":"Generate forks by passing keyword arguments to a constructor. In the snippet below men , women , and nonbinary are branch names. Branch values can be anything, though they will usually be arrays. Provide any number of branches with any names and access their values like object members, as shown below. Info When working with specific backends , branch values are internally converted to appropriate data types (e.g., arrays or tensors). import fairbench as fb import numpy as np sensitive = fb . Fork ( men = np . array ([ 1 , 1 , 0 , 0 , 0 ]), women = np . array ([ 0 , 0 , 1 , 1 , 0 ]), nonbinary = np . array ([ 0 , 0 , 0 , 0 , 1 ])) print ( sensitive . nonbinary ) # [0, 0, 0, 0, 1] To set some (or all) branches programmatically pass a dictionary as a positional argument like so: sensitive = fb . Fork ({ \"non-binary\" : np . array ([ 0 , 0 , 0 , 0 , 1 ])}, men = np . array ([ 1 , 1 , 0 , 0 , 0 ]), women = np . array ([ 0 , 0 , 1 , 1 , 0 ])) To create forks by analysing categorical values found in iterables prepend the latter with the categories@ operator. For example, the following code snippet creates two branches genderMan,genderWoman and stores binary membership to each of those. fork = fb . Fork ( gender = fb . categories @ [ \"Man\" , \"Woman\" , \"Man\" , \"Woman\" , \"Nonbin\" ]) print ( fork ) # genderMan: [1, 0, 1, 0, 0] # genderWoman: [0, 1, 0, 1, 0] # genderNonbin: [0, 0, 0, 0, 1] Add the outcomes of any number of category analyses to a fork. Use positional arguments (instead of named keyword arguments, such as gender in the above example) to avoid prepending the keyword argument's name to branch names. Any iterable can be analysed into categories instead of a list, including categorical tensors or arrays.","title":"Fork definition"},{"location":"basics/forks/#intersectional-analysis","text":"For more than one sensitive attribute, add the branches you would declare for every attribute more branches of the same fork. For instance, the following is a valid fork that considers three gender attribute values and one binary sensitive attribute value for young vs old people: import numpy as np sensitive = fb . Fork ( fb . categories @ [ \"Man\" , \"Woman\" , \"Man\" , \"Woman\" , \"Nonbin\" ], IsOld = np . array ([ 0 , 1 , 0 , 1 , 0 ])) # Man: [1, 0, 1, 0, 0] # Woman: [0, 1, 0, 1, 0] # Nonbin: [0, 0, 0, 0, 1] # IsOld0.0 [1, 0, 0, 0, 1] # IsOld1.0 [0, 1, 1, 1, 0] For more than one sensitive attributes, branches capturing the values of different attributes will have overlapping non-zeroes. Thus, you might want to consider intersectional instead of more naive definitions of fairness by creating all branch combinations with at least one data sample per: sensitive = sensitive . intersectional ()","title":"Intersectional analysis"},{"location":"basics/interactive/","text":"Interactive visualization Interactive visualization explores complex objects generated with FairBench , such as comparison of report value explanations, or of different algorithms. This is done with a user interface that lets you navigate between various perspectives (see below). The same exploration can be performed programmatically. Perspectives Start visualization Interface Perspectives To delve into complicate comparisons between subgroups, you need to understand the concept of report perspectives. Viewing all values stored in a branch is one perspective (e.g., report.min to view assessments for the worst metric values for that branch). But you can also obtain other perspectives, like viewing all values for the same entry in all branches (e.g., report.tpr to view true positive rates across all branches). Perspectives are equivalent to a combination of tensor access alongside a specific dimension. Info Perspectives as a programming pattern arise from FairBench's concurrent execution principle on fork membership access and a Forklike datatype used internally. The latter is a dictionary whose elements can also be accessed like class members. Code-based exploration Info This paragraph is under construction. Branch explanations Info This paragraph is under construction. Algorithm comparison To compare the same type of reports produced by two different algorithms, you need to create a fork with the reports as its branches. In the example below, we use pygrank to run a normal pagerank algorithm and a fairness-aware adaptation for node recommendation. Any number of algorithms can be assessed with a FairBench reporting mechanism and combined into a fork. In this case, the mechanism of choice is the multireport, used with arguments needed to assess recommendation outcomes. At the end, a fork is created and, although it can be too complicated to show in one figure or table, you can obtain any perspective and visualize that. For example, the snippet below prints a table in the console that compares algorithms in terms of various base auc measure reductions. import pygrank as pg import fairbench as fb \"\"\"load data and set sensitive attribute\"\"\" _ , graph , communities = next ( pg . load_datasets_multiple_communities ([ \"highschool\" ])) train , test = pg . split ( pg . to_signal ( graph , communities [ 0 ]), 0.5 ) sensitive_signal = pg . to_signal ( graph , communities [ 1 ]) labels = test . filter ( exclude = train ) sensitive = fb . Fork ( gender = fb . categories @sensitive_signal . filter ( exclude = train )) \"\"\"create report for pagerank\"\"\" algorithm = pg . PageRank ( alpha = 0.85 ) scores = algorithm ( train ) . filter ( exclude = train ) report = fb . multireport ( labels = labels , scores = scores , sensitive = sensitive ) \"\"\"create report for locally fair pagerank\"\"\" fair_algorithm = pg . LFPR ( alpha = 0.85 , redistributor = \"original\" ) fair_scores = fair_algorithm ( train , sensitive = sensitive_signal ) . filter ( exclude = train ) fair_report = fb . multireport ( labels = labels , scores = fair_scores , sensitive = sensitive ) \"\"\"combine both reports into one and get the auc perspective\"\"\" fork = fb . Fork ( ppr = report , lfpr = fair_report ) fb . describe ( fork . auc ) Metric ppr lfpr min 0.680 0.589 wmean 0.780 0.743 minratio 0.792 0.681 maxdiff 0.178 0.276 maxbarea 0.169 0.262 Start visualization To start interactive visualization, call the snippet below on an object that is a dictionary or fork. The call shows the default values for optional arguments, which set a report name to be displayed and whether plots should be horizontally or vertically aligned. Horizontal alignment lets figures remain comprehensive during complex intersectional analysis. import fairbench as fb obj = ... # the object to explore (e.g., a report) fb . interactive ( obj , name = \"report\" , horizontal = True ) When run in a console, the above code will start a bokeh server that hosts a dynamic web page and will open the latter as a tab in your local browser (hard-terminate the process with ctrl+C to stop the server). When run in a Jupyter environment, a bokeh application will start on the next output cell instead. Info When Jupyter runs on its non-default port, add a respective argument (e.g., port=8889 ) to interactive visualization to set appropriate permissions. Interface Interactive visualization starts from the top level of forks/reports and looks like the figures below. Figures will appear only if data objects can be visualized. The first menu option over the figure controls whether to explore data by branches or entries. In the example below, this means by reduction strategy over subgroups or performance metric. You can focus on a specific branch or entry by clicking on its name on buttons over the figure. This will generate the appropriate perspective, as shown below. Two new options are added: a button to go back to the previous perspective and an button to create an explanation perspective. This last button appears only when you focus on specific branches or entries. The current perspective always appears on the top. Currently, we are investigating the minimum reduction strategy of the report. Hover over a bar to view its exact values. Let us now view the explanations of what quantities have led to computing the values of the current figure by clicking on the explain button. The report we are exploring (multireport) performs reduction on the outcome of performance metrics on each sensitive attribute branch, i.e., sensitive attribute dimension. Thus, explanations consist of metric outcomes. Similarly to before, you can switch between showing branches and entries of explanation values. Tip FairBench keeps track of metadata when computing base performance metrics, and you can eventually view them and compare them from within interactive visualization as explanations. For example, below is the explanation for ''true positive rates of the minimum reduction's explanation'', as indicated by reading the perspective's title report.min.explain.tpr.explain from end to start. By looking at base metric explanations, you can get a feel for raw data properties and corresponding systemic, societal, or data gathering issues that eventually give rise to bias.","title":"Interactive visualization"},{"location":"basics/interactive/#interactive-visualization","text":"Interactive visualization explores complex objects generated with FairBench , such as comparison of report value explanations, or of different algorithms. This is done with a user interface that lets you navigate between various perspectives (see below). The same exploration can be performed programmatically. Perspectives Start visualization Interface","title":"Interactive visualization"},{"location":"basics/interactive/#perspectives","text":"To delve into complicate comparisons between subgroups, you need to understand the concept of report perspectives. Viewing all values stored in a branch is one perspective (e.g., report.min to view assessments for the worst metric values for that branch). But you can also obtain other perspectives, like viewing all values for the same entry in all branches (e.g., report.tpr to view true positive rates across all branches). Perspectives are equivalent to a combination of tensor access alongside a specific dimension. Info Perspectives as a programming pattern arise from FairBench's concurrent execution principle on fork membership access and a Forklike datatype used internally. The latter is a dictionary whose elements can also be accessed like class members.","title":"Perspectives"},{"location":"basics/interactive/#code-based-exploration","text":"Info This paragraph is under construction.","title":"Code-based exploration"},{"location":"basics/interactive/#branch-explanations","text":"Info This paragraph is under construction.","title":"Branch explanations"},{"location":"basics/interactive/#algorithm-comparison","text":"To compare the same type of reports produced by two different algorithms, you need to create a fork with the reports as its branches. In the example below, we use pygrank to run a normal pagerank algorithm and a fairness-aware adaptation for node recommendation. Any number of algorithms can be assessed with a FairBench reporting mechanism and combined into a fork. In this case, the mechanism of choice is the multireport, used with arguments needed to assess recommendation outcomes. At the end, a fork is created and, although it can be too complicated to show in one figure or table, you can obtain any perspective and visualize that. For example, the snippet below prints a table in the console that compares algorithms in terms of various base auc measure reductions. import pygrank as pg import fairbench as fb \"\"\"load data and set sensitive attribute\"\"\" _ , graph , communities = next ( pg . load_datasets_multiple_communities ([ \"highschool\" ])) train , test = pg . split ( pg . to_signal ( graph , communities [ 0 ]), 0.5 ) sensitive_signal = pg . to_signal ( graph , communities [ 1 ]) labels = test . filter ( exclude = train ) sensitive = fb . Fork ( gender = fb . categories @sensitive_signal . filter ( exclude = train )) \"\"\"create report for pagerank\"\"\" algorithm = pg . PageRank ( alpha = 0.85 ) scores = algorithm ( train ) . filter ( exclude = train ) report = fb . multireport ( labels = labels , scores = scores , sensitive = sensitive ) \"\"\"create report for locally fair pagerank\"\"\" fair_algorithm = pg . LFPR ( alpha = 0.85 , redistributor = \"original\" ) fair_scores = fair_algorithm ( train , sensitive = sensitive_signal ) . filter ( exclude = train ) fair_report = fb . multireport ( labels = labels , scores = fair_scores , sensitive = sensitive ) \"\"\"combine both reports into one and get the auc perspective\"\"\" fork = fb . Fork ( ppr = report , lfpr = fair_report ) fb . describe ( fork . auc ) Metric ppr lfpr min 0.680 0.589 wmean 0.780 0.743 minratio 0.792 0.681 maxdiff 0.178 0.276 maxbarea 0.169 0.262","title":"Algorithm comparison"},{"location":"basics/interactive/#start-visualization","text":"To start interactive visualization, call the snippet below on an object that is a dictionary or fork. The call shows the default values for optional arguments, which set a report name to be displayed and whether plots should be horizontally or vertically aligned. Horizontal alignment lets figures remain comprehensive during complex intersectional analysis. import fairbench as fb obj = ... # the object to explore (e.g., a report) fb . interactive ( obj , name = \"report\" , horizontal = True ) When run in a console, the above code will start a bokeh server that hosts a dynamic web page and will open the latter as a tab in your local browser (hard-terminate the process with ctrl+C to stop the server). When run in a Jupyter environment, a bokeh application will start on the next output cell instead. Info When Jupyter runs on its non-default port, add a respective argument (e.g., port=8889 ) to interactive visualization to set appropriate permissions.","title":"Start visualization"},{"location":"basics/interactive/#interface","text":"Interactive visualization starts from the top level of forks/reports and looks like the figures below. Figures will appear only if data objects can be visualized. The first menu option over the figure controls whether to explore data by branches or entries. In the example below, this means by reduction strategy over subgroups or performance metric. You can focus on a specific branch or entry by clicking on its name on buttons over the figure. This will generate the appropriate perspective, as shown below. Two new options are added: a button to go back to the previous perspective and an button to create an explanation perspective. This last button appears only when you focus on specific branches or entries. The current perspective always appears on the top. Currently, we are investigating the minimum reduction strategy of the report. Hover over a bar to view its exact values. Let us now view the explanations of what quantities have led to computing the values of the current figure by clicking on the explain button. The report we are exploring (multireport) performs reduction on the outcome of performance metrics on each sensitive attribute branch, i.e., sensitive attribute dimension. Thus, explanations consist of metric outcomes. Similarly to before, you can switch between showing branches and entries of explanation values. Tip FairBench keeps track of metadata when computing base performance metrics, and you can eventually view them and compare them from within interactive visualization as explanations. For example, below is the explanation for ''true positive rates of the minimum reduction's explanation'', as indicated by reading the perspective's title report.min.explain.tpr.explain from end to start. By looking at base metric explanations, you can get a feel for raw data properties and corresponding systemic, societal, or data gathering issues that eventually give rise to bias.","title":"Interface"},{"location":"basics/modelcards/","text":"Stamps and model cards When disseminating models, it is often important to summarize evaluations under specific fairness perspectives, and whether they meet specific thresholds. FairBench calls such evaluations stamps , and allows their automated extraction from reports (reports hold a lot of information, some of which can be matched to speccific popular definitions of fairness to be added to model cards). Several stamps can be aggregated into forks and outputted in various model card formats. Stamps Combine stamps Export to model cards Stamps Stamps are callables that take as inputs reports, extract specific fields, and apply potential threshold checks. Calling a stamp returns an Explainable object (or an ExplainableError in case it cannot be retrieved from the report). This holds either boolean or numeric values, respectively asserting that some property is met and quantifying how well a model performs. Several stamps are provided within the fairbench.stamps backage. To avoid confusion with code library functionalities, stamps are not accessed from the global level. Run a stamp per: import fairbench as fb report = ... stamp = fb . stamps . four_fifths_rule ( report ) print ( stamp ) # 3/4ths ratio: False Combine stamps You may want to check a model for various characteristics, and therefore assess it with various stamps. To do so, combine the stamps into one entity like in the following snippet. Each stamp's value is an Explainable object under the hood, but they all contain different explanation data; avoid mass-explanations (e.g., with stamps.explain ) and instead export collections of stamps to model cards for a more thorough explanation (see below). stamps = fb . combine ( fb . stamps . prule ( report ), fb . stamps . accuracy ( report ), fb . stamps . four_fifths_rule ( report ) ) print ( stamps ) prule: 0.212 worst accuracy: 0.799 4/5 rule: False Export to model cards Combined stamps can be exported in popular formats that are often used to represent model cards. The exports can be treated as fairness model cards or be integrated to the full cards of models being tested. Export methods can produce new files if given a path, but also return the respective string conversion they would write to the file. To avoid confusion with core library functionalities, model card exports can only be accessed from the module fairbench.modelcards and not from the top level. For example, use the following snippet to export markdown: print ( fb . modelcards . tomarkdown ( stamps )) # or toyaml or tohtml The output will be this markdown . Notice that the original stamp outputs keep track of factors, descriptions, and recommendations to add to respective report fields. The fairbench.modelcards.tohtml method can also let you open the generated html in your browser (without necessarily creating a file) like so: fb . modelcards . tohtml ( stamps , show = True )","title":"Stamps and model cards"},{"location":"basics/modelcards/#stamps-and-model-cards","text":"When disseminating models, it is often important to summarize evaluations under specific fairness perspectives, and whether they meet specific thresholds. FairBench calls such evaluations stamps , and allows their automated extraction from reports (reports hold a lot of information, some of which can be matched to speccific popular definitions of fairness to be added to model cards). Several stamps can be aggregated into forks and outputted in various model card formats. Stamps Combine stamps Export to model cards","title":"Stamps and model cards"},{"location":"basics/modelcards/#stamps","text":"Stamps are callables that take as inputs reports, extract specific fields, and apply potential threshold checks. Calling a stamp returns an Explainable object (or an ExplainableError in case it cannot be retrieved from the report). This holds either boolean or numeric values, respectively asserting that some property is met and quantifying how well a model performs. Several stamps are provided within the fairbench.stamps backage. To avoid confusion with code library functionalities, stamps are not accessed from the global level. Run a stamp per: import fairbench as fb report = ... stamp = fb . stamps . four_fifths_rule ( report ) print ( stamp ) # 3/4ths ratio: False","title":"Stamps"},{"location":"basics/modelcards/#combine-stamps","text":"You may want to check a model for various characteristics, and therefore assess it with various stamps. To do so, combine the stamps into one entity like in the following snippet. Each stamp's value is an Explainable object under the hood, but they all contain different explanation data; avoid mass-explanations (e.g., with stamps.explain ) and instead export collections of stamps to model cards for a more thorough explanation (see below). stamps = fb . combine ( fb . stamps . prule ( report ), fb . stamps . accuracy ( report ), fb . stamps . four_fifths_rule ( report ) ) print ( stamps ) prule: 0.212 worst accuracy: 0.799 4/5 rule: False","title":"Combine stamps"},{"location":"basics/modelcards/#export-to-model-cards","text":"Combined stamps can be exported in popular formats that are often used to represent model cards. The exports can be treated as fairness model cards or be integrated to the full cards of models being tested. Export methods can produce new files if given a path, but also return the respective string conversion they would write to the file. To avoid confusion with core library functionalities, model card exports can only be accessed from the module fairbench.modelcards and not from the top level. For example, use the following snippet to export markdown: print ( fb . modelcards . tomarkdown ( stamps )) # or toyaml or tohtml The output will be this markdown . Notice that the original stamp outputs keep track of factors, descriptions, and recommendations to add to respective report fields. The fairbench.modelcards.tohtml method can also let you open the generated html in your browser (without necessarily creating a file) like so: fb . modelcards . tohtml ( stamps , show = True )","title":"Export to model cards"},{"location":"basics/reports/","text":"Reports Reports perform multi-faceted analyses of system outcomes (e.g., predictions, recommendations, regression scores) and produce explainable high-level views of biases found across several definitions of fairness. Generate reports Report types Show reports Explainable values Tip To check for specific fairness assessments from the literature, produce reports that contain relevant information (e.g., multireports shown below) and extract from them appropriate stamps . Generate reports You can generate fairness reports by providing some of the optional arguments below to a report generation method. These are needed to compute performance metrics . Report generation method will try to compute fairness assessment built from as many base metrics as they can, depending on which arguments are provided. You can also provide an optional metrics argument that holds either a dictionary mapping names to metrics or a list of metrics, where in the last case their names are automatically inferred. Sensitive attributes are forks to handle multi-value attributes or multiple sensitive attribute values. Argument Role Values predictions system output binary array scores system output array with elements in [0,1] targets prediction target array with elements in [0,1] labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in [0,1] (either binary or fuzzy) Info For the time being, for multiclass problems you need to perform a different fairness assessment for each for class label, but this will change in the future with new capabilities. Report types Out-of-the box, you can use one of the following three report generation methods: accreport provides popular performance evaluation measures to be viewed between branches (accuracy, positives, true positive rates, true negative rates). binreport conducts a suit of popular binary fairness assessments on each variable branch and should be preferred when branches do not correspond to mult-attribute fairness. multireport is ideal for multi-fairness approaches, where the sensitive fork has many branches. This report generator performs a lot of editing to summarize the findings of multi-attribute fairness analysis. isecreport tackles mult-fairness with many intersectional groups. Its output approximates multireport with a Bayesian framework that is applicable even when protected group intersections are too small to yield meaningful predictions. As an example, let's create a simple report based on binary predictions, binary ideal predictions and multiclass sensitive attribute sensitive , which is declared to be a fork with two branches men,women , each of which is a binary feature value per: import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . multireport ( predictions =... , labels =... , sensitive = sensitive ) Show reports Report are forks whose branches hold dictionaries of metric computations. In some reports (e.g., multireport) reduction operations introduce a comparative analysis between the sensitive attribute branches to investigate unfairness. For example, min shows the worst evaluation across sensitive groups, and minratio and maxdiff the minimum ratio and maximum differences between metric values for sensitive groups (groups correspond to sensitive attribute branches). Several methods are provided to work with the report data format, namely forks of dictionaries. First, you can show reports in the stdout console in the form of tables: fb . describe ( report ) Metric min minratio maxdiff accuracy 0.938 1.000 0.000 pr 0.812 0.857 0.125 fpr 0.063 0.778 0.016 fnr 0.333 0.333 0.333 You can also convert reports to json , for example to send to some frontend: print ( fb . tojson ( report )) { \"header\" : [ \"Metric\" , \"mean\" , \"minratio\" , \"maxdiff\" ], \"accuracy\" : [ 0.9375 , 1.0 , 0.0 ], \"pr\" : [ 0.8125 , 0.8571428571428571 , 0.125 ], \"fpr\" : [ 0.06349206349206349 , 0.7777777777777778 , 0.015873015873015872 ], \"fnr\" : [ 0.3333333333333333 , 0.3333333333333333 , 0.33333333333333337 ]} Reports can be visualized with matplotlib : fb . visualize ( report ) Warning Not all forks can be parsed by fb.visualize and fb.display . For example, you cannot visualize a fork of reports. Explore complicated forks with interactive visualization . Explainable values Some report values, can be explained in terms of data they are derived from. For instance, if a fairbench.isecreport is made, both empirical and bayesian evaluations arise from the underlying data branches of multi-attribute fairness forks. Whenever possible, the data branches that are converted into final reports are preserved by having report values be instances of the Explainable class. This provides an .explain field of data contributing to the report value, and .desc field to store additional descriptions. You can perform arithmetic operations between explainable objects and other numbers and the outcome will be normal python numbers. As an example, below we use these fields to retrieve posterior estimations contributing to calculating the baysian branch of the minprule metric in the isecreport . report = fb . isecreport ( vals ) fb . describe ( report ) fb . describe ( report . bayesian . minprule . explain ) Metric empirical bayesian minprule 0.857 0.853 Metric case1 case2 case2,case1 0.729 0.706 0.827","title":"Reports"},{"location":"basics/reports/#reports","text":"Reports perform multi-faceted analyses of system outcomes (e.g., predictions, recommendations, regression scores) and produce explainable high-level views of biases found across several definitions of fairness. Generate reports Report types Show reports Explainable values Tip To check for specific fairness assessments from the literature, produce reports that contain relevant information (e.g., multireports shown below) and extract from them appropriate stamps .","title":"Reports"},{"location":"basics/reports/#generate-reports","text":"You can generate fairness reports by providing some of the optional arguments below to a report generation method. These are needed to compute performance metrics . Report generation method will try to compute fairness assessment built from as many base metrics as they can, depending on which arguments are provided. You can also provide an optional metrics argument that holds either a dictionary mapping names to metrics or a list of metrics, where in the last case their names are automatically inferred. Sensitive attributes are forks to handle multi-value attributes or multiple sensitive attribute values. Argument Role Values predictions system output binary array scores system output array with elements in [0,1] targets prediction target array with elements in [0,1] labels prediction target binary array sensitive sensitive attribute fork of arrays with elements in [0,1] (either binary or fuzzy) Info For the time being, for multiclass problems you need to perform a different fairness assessment for each for class label, but this will change in the future with new capabilities.","title":"Generate reports"},{"location":"basics/reports/#report-types","text":"Out-of-the box, you can use one of the following three report generation methods: accreport provides popular performance evaluation measures to be viewed between branches (accuracy, positives, true positive rates, true negative rates). binreport conducts a suit of popular binary fairness assessments on each variable branch and should be preferred when branches do not correspond to mult-attribute fairness. multireport is ideal for multi-fairness approaches, where the sensitive fork has many branches. This report generator performs a lot of editing to summarize the findings of multi-attribute fairness analysis. isecreport tackles mult-fairness with many intersectional groups. Its output approximates multireport with a Bayesian framework that is applicable even when protected group intersections are too small to yield meaningful predictions. As an example, let's create a simple report based on binary predictions, binary ideal predictions and multiclass sensitive attribute sensitive , which is declared to be a fork with two branches men,women , each of which is a binary feature value per: import fairbench as fb sensitive = fb . Fork ( case1 =... , case2 =... ) report = fb . multireport ( predictions =... , labels =... , sensitive = sensitive )","title":"Report types"},{"location":"basics/reports/#show-reports","text":"Report are forks whose branches hold dictionaries of metric computations. In some reports (e.g., multireport) reduction operations introduce a comparative analysis between the sensitive attribute branches to investigate unfairness. For example, min shows the worst evaluation across sensitive groups, and minratio and maxdiff the minimum ratio and maximum differences between metric values for sensitive groups (groups correspond to sensitive attribute branches). Several methods are provided to work with the report data format, namely forks of dictionaries. First, you can show reports in the stdout console in the form of tables: fb . describe ( report ) Metric min minratio maxdiff accuracy 0.938 1.000 0.000 pr 0.812 0.857 0.125 fpr 0.063 0.778 0.016 fnr 0.333 0.333 0.333 You can also convert reports to json , for example to send to some frontend: print ( fb . tojson ( report )) { \"header\" : [ \"Metric\" , \"mean\" , \"minratio\" , \"maxdiff\" ], \"accuracy\" : [ 0.9375 , 1.0 , 0.0 ], \"pr\" : [ 0.8125 , 0.8571428571428571 , 0.125 ], \"fpr\" : [ 0.06349206349206349 , 0.7777777777777778 , 0.015873015873015872 ], \"fnr\" : [ 0.3333333333333333 , 0.3333333333333333 , 0.33333333333333337 ]} Reports can be visualized with matplotlib : fb . visualize ( report ) Warning Not all forks can be parsed by fb.visualize and fb.display . For example, you cannot visualize a fork of reports. Explore complicated forks with interactive visualization .","title":"Show reports"},{"location":"basics/reports/#explainable-values","text":"Some report values, can be explained in terms of data they are derived from. For instance, if a fairbench.isecreport is made, both empirical and bayesian evaluations arise from the underlying data branches of multi-attribute fairness forks. Whenever possible, the data branches that are converted into final reports are preserved by having report values be instances of the Explainable class. This provides an .explain field of data contributing to the report value, and .desc field to store additional descriptions. You can perform arithmetic operations between explainable objects and other numbers and the outcome will be normal python numbers. As an example, below we use these fields to retrieve posterior estimations contributing to calculating the baysian branch of the minprule metric in the isecreport . report = fb . isecreport ( vals ) fb . describe ( report ) fb . describe ( report . bayesian . minprule . explain ) Metric empirical bayesian minprule 0.857 0.853 Metric case1 case2 case2,case1 0.729 0.706 0.827","title":"Explainable values"},{"location":"images/example_modelcard/","text":"Factors The groups that are considered for fairness assessment are Female, Male. Metrics Fairness-aware metrics are computed. The p-rule compares the fraction of positive predictions between groups. The worst ratio is reported, so that value of 0 indicates disparate impact, and value of 1 disparate impact mitigation. The worst accuracy computes the worst performance among protected groups; this is the minimum benefit the system brings to any group. The \u2158 rule checks whether the fraction of positive predictions for each protected group is at worst four fifths that of any other group (i.e., the p-rule is 0.8 or greater). Evaluation Results Metric Value p-rule 0.212 worst accuracy 0.799 \u2158 rule Caveats and Recommendations Consider input from affected stakeholders to determine whether the 80% rule is an appropriate fairness criterion. Disparate impact may not always be an appropriate fairness consideration. The worst accuracy is a lower bound but not an estimation of overall accuracy. There may be different distributions of benefits that could be protected.","title":"Example modelcard"},{"location":"images/example_modelcard/#factors","text":"The groups that are considered for fairness assessment are Female, Male.","title":"Factors"},{"location":"images/example_modelcard/#metrics","text":"Fairness-aware metrics are computed. The p-rule compares the fraction of positive predictions between groups. The worst ratio is reported, so that value of 0 indicates disparate impact, and value of 1 disparate impact mitigation. The worst accuracy computes the worst performance among protected groups; this is the minimum benefit the system brings to any group. The \u2158 rule checks whether the fraction of positive predictions for each protected group is at worst four fifths that of any other group (i.e., the p-rule is 0.8 or greater).","title":"Metrics"},{"location":"images/example_modelcard/#evaluation-results","text":"Metric Value p-rule 0.212 worst accuracy 0.799 \u2158 rule","title":"Evaluation Results"},{"location":"images/example_modelcard/#caveats-and-recommendations","text":"Consider input from affected stakeholders to determine whether the 80% rule is an appropriate fairness criterion. Disparate impact may not always be an appropriate fairness consideration. The worst accuracy is a lower bound but not an estimation of overall accuracy. There may be different distributions of benefits that could be protected.","title":"Caveats and Recommendations"}]}